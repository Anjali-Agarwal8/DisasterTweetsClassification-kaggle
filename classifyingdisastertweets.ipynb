{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:24.923929Z",
     "iopub.status.busy": "2021-01-24T17:38:24.923114Z",
     "iopub.status.idle": "2021-01-24T17:38:24.930987Z",
     "shell.execute_reply": "2021-01-24T17:38:24.930032Z"
    },
    "papermill": {
     "duration": 0.027924,
     "end_time": "2021-01-24T17:38:24.931156",
     "exception": false,
     "start_time": "2021-01-24T17:38:24.903232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:24.955225Z",
     "iopub.status.busy": "2021-01-24T17:38:24.954582Z",
     "iopub.status.idle": "2021-01-24T17:38:25.063164Z",
     "shell.execute_reply": "2021-01-24T17:38:25.062608Z"
    },
    "papermill": {
     "duration": 0.120436,
     "end_time": "2021-01-24T17:38:25.063288",
     "exception": false,
     "start_time": "2021-01-24T17:38:24.942852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets  4342\n",
      "Number of negative tweets  3271\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "print(\"Number of positive tweets \",len(train[train[\"target\"] == 0]))\n",
    "print(\"Number of negative tweets \",len(train[train[\"target\"] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:25.095407Z",
     "iopub.status.busy": "2021-01-24T17:38:25.091822Z",
     "iopub.status.idle": "2021-01-24T17:38:26.995296Z",
     "shell.execute_reply": "2021-01-24T17:38:26.994605Z"
    },
    "papermill": {
     "duration": 1.922602,
     "end_time": "2021-01-24T17:38:26.995402",
     "exception": false,
     "start_time": "2021-01-24T17:38:25.072800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "def preprocess(tweet):\n",
    "    \n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet) # remove Retweet text\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet) # remove hyperlinks\n",
    "    tweet = re.sub(r'#', '', tweet) #remove hashtags\n",
    "    \n",
    "    #Tokenizing\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False,reduce_len=True,strip_handles=True)\n",
    "    tokenized_tweet = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    #Removing stopwords and punctuation. Stemming the remaining words.\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    processed_tweet = []\n",
    "    for word in tokenized_tweet:\n",
    "        if(word not in english_stopwords and word not in string.punctuation):\n",
    "            processed_tweet.append(stemmer.stem(word))\n",
    "            \n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:27.028793Z",
     "iopub.status.busy": "2021-01-24T17:38:27.024462Z",
     "iopub.status.idle": "2021-01-24T17:38:27.043136Z",
     "shell.execute_reply": "2021-01-24T17:38:27.043831Z"
    },
    "papermill": {
     "duration": 0.038797,
     "end_time": "2021-01-24T17:38:27.044017",
     "exception": false,
     "start_time": "2021-01-24T17:38:27.005220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet:  #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
      "Processed Tweet:  ['rockyfir', 'updat', 'california', 'hwi', '20', 'close', 'direct', 'due', 'lake', 'counti', 'fire', 'cafir', 'wildfir']\n"
     ]
    }
   ],
   "source": [
    "#Verifying \n",
    "print(\"Original Tweet: \",train[\"text\"][5])\n",
    "print(\"Processed Tweet: \",preprocess(train[\"text\"][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:27.075645Z",
     "iopub.status.busy": "2021-01-24T17:38:27.074962Z",
     "iopub.status.idle": "2021-01-24T17:38:27.079812Z",
     "shell.execute_reply": "2021-01-24T17:38:27.079114Z"
    },
    "papermill": {
     "duration": 0.02512,
     "end_time": "2021-01-24T17:38:27.079926",
     "exception": false,
     "start_time": "2021-01-24T17:38:27.054806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Building frequency dictionary of every unique word in the dataset\n",
    "def freq_builder(tweets,labels):\n",
    "    freq_dict = {}\n",
    "    labels_list = np.squeeze(labels).tolist()\n",
    "    for tweet,l in zip(tweets,labels_list):\n",
    "        for word in preprocess(tweet):\n",
    "            pair = (word,l)\n",
    "            if(pair in freq_dict):\n",
    "                freq_dict[pair] += 1\n",
    "            else:\n",
    "                freq_dict[pair] =  1\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:27.114640Z",
     "iopub.status.busy": "2021-01-24T17:38:27.110779Z",
     "iopub.status.idle": "2021-01-24T17:38:35.164690Z",
     "shell.execute_reply": "2021-01-24T17:38:35.165353Z"
    },
    "papermill": {
     "duration": 8.075155,
     "end_time": "2021-01-24T17:38:35.165519",
     "exception": false,
     "start_time": "2021-01-24T17:38:27.090364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('forgiv', 1), 1), (('us', 1), 44), (('forest', 1), 48), (('fire', 1), 268), (('near', 1), 49)]\n"
     ]
    }
   ],
   "source": [
    "#Creating the frequency dictionary using the dataset\n",
    "frequency = freq_builder(train[\"text\"],train[\"target\"])\n",
    "print(list(frequency.items())[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:35.204933Z",
     "iopub.status.busy": "2021-01-24T17:38:35.204221Z",
     "iopub.status.idle": "2021-01-24T17:38:35.207551Z",
     "shell.execute_reply": "2021-01-24T17:38:35.206870Z"
    },
    "papermill": {
     "duration": 0.026846,
     "end_time": "2021-01-24T17:38:35.207663",
     "exception": false,
     "start_time": "2021-01-24T17:38:35.180817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Extracting features from the frequency dictionary for the dataset. For each tweet first feature will be\n",
    "#number of words related to disaster in the tweet and second feature will be number of words not related \n",
    "#to disaster.\n",
    "def feature_extraction(tweet,freq_dict):\n",
    "    tokenized_tweet = preprocess(tweet)\n",
    "    f = np.zeros((1,3))\n",
    "    f[0,0] = 1 #bias term\n",
    "    for word in tokenized_tweet:\n",
    "        f[0,1] += freq_dict.get((word,1.0),0)\n",
    "        f[0,2] += freq_dict.get((word,0.0),0)\n",
    "        \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:35.240767Z",
     "iopub.status.busy": "2021-01-24T17:38:35.240047Z",
     "iopub.status.idle": "2021-01-24T17:38:40.569633Z",
     "shell.execute_reply": "2021-01-24T17:38:40.570784Z"
    },
    "papermill": {
     "duration": 5.352245,
     "end_time": "2021-01-24T17:38:40.570969",
     "exception": false,
     "start_time": "2021-01-24T17:38:35.218724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1. 266. 192.]\n",
      " [  1. 502. 492.]\n",
      " [  1. 240. 216.]\n",
      " [  1. 149.  54.]\n",
      " [  1. 319. 196.]]\n",
      "10    1\n",
      "11    1\n",
      "12    1\n",
      "13    1\n",
      "14    1\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Extracting features from dataset\n",
    "X_train = np.zeros((len(train[\"target\"]),3))\n",
    "for i in range(len(train[\"target\"])):\n",
    "    X_train[i,:] = feature_extraction(train[\"text\"][i],frequency)\n",
    "Y_train = train[\"target\"]\n",
    "\n",
    "print(X_train[10:15])\n",
    "print(Y_train[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:40.613679Z",
     "iopub.status.busy": "2021-01-24T17:38:40.612992Z",
     "iopub.status.idle": "2021-01-24T17:38:49.377030Z",
     "shell.execute_reply": "2021-01-24T17:38:49.377894Z"
    },
    "papermill": {
     "duration": 8.790185,
     "end_time": "2021-01-24T17:38:49.378034",
     "exception": false,
     "start_time": "2021-01-24T17:38:40.587849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score:  [0.78167488 0.78995074 0.77029157]\n",
      "Test Accuracy Score:  [0.76989756 0.76201734 0.80134017]\n",
      "Train F1 Score:  [0.7186389  0.7272262  0.70133197]\n",
      "Test F1 Score:  [0.69927909 0.70069376 0.73858921]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#lr = LogisticRegression(penalty='l2',C = 1.0, solver = 'lbfgs', max_iter=100)\n",
    "#result = cross_validate(lr, X_train, Y_train, scoring = ('accuracy','f1'), cv = 3, return_train_score=True)\n",
    "svm = SVC(C=1.0,kernel='rbf')\n",
    "result = cross_validate(svm,X_train,Y_train,scoring = ('accuracy','f1'), cv = 3, return_train_score=True)\n",
    "print(\"Train Accuracy Score: \",result[\"train_accuracy\"])\n",
    "print(\"Test Accuracy Score: \",result[\"test_accuracy\"])\n",
    "print(\"Train F1 Score: \",result[\"train_f1\"])\n",
    "print(\"Test F1 Score: \",result[\"test_f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-24T17:38:49.411969Z",
     "iopub.status.busy": "2021-01-24T17:38:49.411128Z",
     "iopub.status.idle": "2021-01-24T17:38:53.785291Z",
     "shell.execute_reply": "2021-01-24T17:38:53.783160Z"
    },
    "papermill": {
     "duration": 4.395394,
     "end_time": "2021-01-24T17:38:53.785438",
     "exception": false,
     "start_time": "2021-01-24T17:38:49.390044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generating the Submission file\n",
    "X_test = np.zeros((len(test[\"text\"]),3))\n",
    "#lr = LogisticRegression(penalty='l2',C = 1.0, solver = 'lbfgs', max_iter=100).fit(X_train,Y_train)\n",
    "model = SVC(C=1.0,kernel='rbf').fit(X_train,Y_train)\n",
    "for i in range(len(test[\"text\"])):\n",
    "    X_test[i,:] = feature_extraction(test[\"text\"][i],frequency)\n",
    "Y_test = model.predict(X_test)\n",
    "submission_df = pd.DataFrame({'id' : test[\"id\"],'target' : Y_test})\n",
    "submission = submission_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011073,
     "end_time": "2021-01-24T17:38:53.808400",
     "exception": false,
     "start_time": "2021-01-24T17:38:53.797327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 34.552012,
   "end_time": "2021-01-24T17:38:53.928210",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-24T17:38:19.376198",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
